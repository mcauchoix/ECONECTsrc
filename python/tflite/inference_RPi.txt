# Nous avons essayé 2 modèles SSD :
# 1) SSD MobileNet V2 320x320 (taille des images en entrée)
# 2) SSD MobileNet V2 FPNLite 640x640 (taille des images en entrée)
# => Pour l'embarqué, le premier modèle est recommandé car il a de meilleures performances (et 5 fois plus rapide) 
# Ces deux modèles ont chacun un fichier qui permet de les utiliser :
# 1) SSD MobileNet V2 320x320  	      => models/modelV2.tflite    # TODO nouvel entraînement CALMIP sur les données CVAT du 07/09/2021
# 2) SSD MobileNet V2 FPNLite 640x640 => models/modelV8.tflite

*************************************************************
 ******** Création de l'environnement virtuel Conda ********
*************************************************************
# ******** A executer une seule fois ******** #
sudo apt-get update
sudo apt-get dist-upgrade
sudo raspi-config  --> activer la caméra et SSH

# ******** Récupération de l'environnement de travail ******** #
cd Documents
git clone https://github.com/mcauchoix/ECONECTsrc.git
mv ECONECTsrc/python/tflite $PWD/tflite
rm -rf ECONECTsrc
# On peut mettre de nouveaux fichiers .tflite et labelmap.txt dans /home/Documents/tflite/models

# ******** Création de l'environnement virtuel : tflite ******** #
sudo pip3 install virtualenv
python3 -m venv tflite
# ******** Fin section ******** #

# A chaque fois qu'on ouvre un terminal
cd Documents/tflite
source bin/activate
bash install-prerequisites.sh

python -c "import tflite_runtime as tf;print(tf.__version__)"  --> Devrait afficher 2.5.0

# Récupérer le chemin vers le dossier d'images/vidéos à détecter pour les programmes Python ci-dessous.

*************************************************************
   ******** Utilisation des modèles en embarqué ********
*************************************************************
# A chaque fois qu'on ouvre un terminal
cd Documents/tflite
source bin/activate

# Pour l'inférence sur les images de test : (inspiré de https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi)
cd Documents/tflite
python3 TFLite_detection_image.py --modeldir=models/modelV2.tflite --imagedir=test

# Inférence sur une vidéo avec le SSD 320x320 (TrainV2), avec les détections affichés en temps réel :
cd Documents/tflite
python3 TFLite-Video-od.py --model=models/modelV2.tflite --labels=models/labelmap.txt --video=videos/2021-09-16-09-18-27.mp4

# Inférence sur une vidéo avec le SSD 640x640 FPN Lite (TrainV8) avec les détections affichés en temps réel :
cd Documents/tflite
python3 TFLite-Video-od.py --model=models/modelV8.tflite --labels=models/labelmap.txt --video=videos/2021-09-16-09-18-27.mp4

# Inférence sur une vidéo avec le SD 320x320 (TrainV2) avec les détections sauvegardées :
cd Documents/tflite
python3 TFLite-VideoFrames-od.py  --model=models/modelV2.tflite --labels=models/labelmap.txt --video=videos/2021-09-16-09-18-27.mp4

# Pour évaluer le modèle lite :
cd Documents/tflite
python3 TFLite_evaluation.py --modeldir=models --imagedir=test

*************************************************************
   ******** Liste des paramètres pour les scripts ********
*************************************************************
TFLite_detection_image.py : (avec des exemples au dessus)
 1) Obligatoires : 
	--modeldir : Dossier où le fichier .tflite du modèle est situé. 
	--labels : Dossier où le fichier labelmap (.txt) est situé.
	--imagedir : Dossier où sont situées les images seulement (.JPG)
 2) Optionnels  :
	--threshold : Seuil minimum de confiance pour afficher les objets (0.65 par défaut)
	--image : Chemin vers l'unique image à détecter. Si on veut lancer la détection sur un dossier d'images, utiliser l'option --imagedir
	--edgetpu : pour utiliser un Coral Edge TPU Accelerator

TFLite-Video-od.py : 
 1) Obligatoires : 
	--modeldir : Dossier où le fichier .tflite du modèle est situé. 
	--labels : Dossier où le fichier labelmap (.txt) est situé.
	--video : Dossier où la vidéo à détecter se situé. (exemple : videos/2021-09-16-09-18-27.mp4)
 2) Optionnels  :
	--threshold : Seuil minimum de confiance pour afficher les objets (0.65 par défaut)

TFLite-VideoFrames-od.py : mêmes paramètres que pour TFLite-Video-od.py

TFLite-PiCamera-od.py :
 1) Obligatoires : mêmes paramètres Obligatoires que pour TFLite-Video-od.py
 2) Optionnels  :
	--resolution : Résolution (longueur et largeur). Erreur possible si la caméra ne prend pas en charge la résolution. (1920x1088 par défaut)

TFLite_evaluation.py : mêmes paramètres que pour TFLite_detection_image.py

*************************************************************
 *** Liste des scripts (avec leurs paramètres ci-dessus) ***
*************************************************************
TFLite_detection_image.py : Détection sur un répertoire qui contient des images .JPG : Sauvegarde les détections sur les images.
TFLite-Video-od.py : Détection sur une seule vidéo (.mp4 ou autre selon opencv) en paramètre : Compte et affiche les détections à la volée.
TFLite-VideoFrames-od.py : Détection sur une seule vidéo (.mp4 ou autre selon opencv) en paramètre : Compte et sauvegarde les détections (frame par frame).
			   Ces frames sont sauvegardées dans un dossier "saved_frames" situé au même endroit que le script. Ce dossier contient
		           un sous-dossier pour chaque vidéo à traiter.

TFLite-PiCamera-od.py : Utilise la Pi Camera pour détecter en temps réel : Compte et affiche les détections à la volée.
TODO TFLite_detection_stream.py : Utilisation de Thread pour augmenter les FPS de la Pi Camera pour détecter en temps réel : Compte et affiche les détections à la volée.

TFLite_evaluation.py : Génère un fichier CSV (au même endroit que le script) pour calculer par la suite les métriques de performances (précision et recall) de chaque espèce. Cela
permet d'évaluer les entraînements de nos 2 modèles sur GPU avec le script tf2_matrice_confusion.py (car plus rapide). Les résultats sont sauvegardés
manuellement dans un fichier Excel nommé "Precision_Recall_FromCSVManuel.xlsx".
   